{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1.e-4\n",
    "WEIGHT_DECAY = 1.e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        # Shared backbone\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False) # the result is 16*2*2\n",
    "        self.size = 2*2*16\n",
    "        self.fc = nn.Linear(self.size, 32)\n",
    "        \n",
    "        # Layery for the policy (action)\n",
    "        self.fc_action_1 = nn.Linear(32, 16)\n",
    "        self.fc_action_2 = nn.Linear(16, 9)\n",
    "        \n",
    "        # Layers for the critic (value)\n",
    "        self.fc_value_1 = nn.Linear(32, 8)\n",
    "        self.fc_value_2 = nn.Linear(8, 1)\n",
    "        self.tanh_value = nn.Tanh() # value will be between -1 and 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone process\n",
    "        y = F.relu(self.conv(x)) # the output has shape (batch_size 2, 2, 16), needs reshaping\n",
    "        y = y.view(-1, self.size) # reshaping\n",
    "        y = F.relu(self.fc(y))\n",
    "        \n",
    "        # Policy head\n",
    "        a = F.relu(self.fc_action_1(y))\n",
    "        a = self.fc_action_2(a)\n",
    "        \n",
    "        # remove unavailable actions\n",
    "        avail = (torch.abs(x.squeeze())!=1).type(torch.FloatTensor).view(-1, 9)\n",
    "        \n",
    "        # subtract off max for numerical stability (avoids large numbers when taking the exponential)\n",
    "        max_a = torch.max(a)\n",
    "        stable = a-max_a \n",
    "        \n",
    "        # Softmax only on available actions\n",
    "        exp = torch.exp(stable)\n",
    "        exp = avail*stable # zero out unavailable actions\n",
    "        prob = exp/torch.sum(exp) # normalize -> SOFTMAX\n",
    "        \n",
    "        # Critic head\n",
    "        value = F.relu(self.fc_value_1(y))\n",
    "        value = self.tanh_value(self.fc_value_2(value))\n",
    "        \n",
    "        return prob.view(3,3), value    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players Definition: \n",
    "\n",
    "1. Random Player\n",
    "2. MCTS NN Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MCTS\n",
    "from copy import copy\n",
    "import random\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_actions())\n",
    "\n",
    "def Policy_Player_MCTS(game, policy, explore_steps=50):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(explore_steps):\n",
    "        mytree.explore(policy)\n",
    "        \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "\n",
    "    return mytreenext.game.last_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    }
   ],
   "source": [
    "# Progress bar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_turn(tree, policy, explore_steps=50):\n",
    "    for _ in range(50):\n",
    "        tree.explore(policy) # does not return anything. Only performs the exploration, and updates the tree\n",
    "    player = tree.game.player\n",
    "    next_tree, (v, nn_v, p, nn_p) = tree.next(temperature=0.1) # chooses the action, and returns the next tree\n",
    "    return next_tree, (v, nn_v, p, nn_p), player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the policy, game and oprimizer\n",
    "import torch.optim as optim\n",
    "from ConnectN import ConnectN\n",
    "\n",
    "game_setting = {'size':(3,3), 'N':3}\n",
    "# game = ConnectN(**game_setting)\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=LR, weight_decay=1.e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# from collections import deque\n",
    "\n",
    "def train(game_setting, policy, episodes, explore_steps=50):\n",
    "    outcomes = []\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        # mytree = MCTS.Node(ConnectN(**game_setting)) # New tree for each episode, receives a new game each time\n",
    "        new_game = ConnectN(**game_setting)\n",
    "        mytree = MCTS.Node(new_game)\n",
    "        \n",
    "        vterm = []\n",
    "        logterm = []\n",
    "        while mytree.outcome is None: # Play until the end of the game\n",
    "            mytree, (v, nn_v, p, nn_p), current_player = take_turn(mytree, policy, explore_steps=explore_steps)\n",
    "            mytree.detach_mother()\n",
    "            \n",
    "            # Compute log_prob ---------> logterm = - sum(log(nn_p) - p*log(p))\n",
    "            loglist = torch.log(nn_p)\n",
    "            constant = torch.where(p>0, p*torch.log(p), torch.tensor(0.))\n",
    "            logterm.append(-torch.sum(loglist-constant))\n",
    "            \n",
    "            vterm.append(nn_v*current_player) # adjust sign of vterm for each player\n",
    "            \n",
    "        outcome = mytree.outcome\n",
    "        outcomes.append(outcome)\n",
    "        \n",
    "        loss = torch.sum((torch.stack(vterm)-outcome)**2 + torch.stack(logterm)) # value + policy losses\n",
    "        optimizer.zero_grad() # clean the gradients\n",
    "        loss.backward() # Calculates the gradients\n",
    "        losses.append(float(loss))\n",
    "        optimizer.step() # update the parameters of the model\n",
    "        \n",
    "        if (e+1)%50==0:\n",
    "            print(\"game: \",e+1, \", mean loss: {:3.2f}\".format(np.mean(losses[-20:])),\n",
    "                \", recent outcomes: \", outcomes[-10:])\n",
    "            \n",
    "        del loss\n",
    "        \n",
    "        timer.update(e+1)\n",
    "    \n",
    "    timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "episodes = 400\n",
    "\n",
    "train(game_setting, policy, episodes, explore_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
